import os
import pandas as pd
import shutil
import matplotlib.pyplot as plt
import time

# -----------------------------------------------------------------------------
# This script scans specified drives (e.g., C:\ and D:\), calculates the size of each 
# top‚Äêlevel folder in gigabytes, logs progress to a UTF‚Äê8 text file, and compiles the 
# results into a CSV and a bar chart of the top 10 largest folders.
#
# Requirements:
# - Python 3.x (includes built-in modules: os, shutil, time, csv)
# - pandas (install via pip: pip install pandas)
# - matplotlib (install via pip: pip install matplotlib)
#
# Setup:
# 1. Ensure pandas and matplotlib are installed.
# 2. Edit the `drives` list to include any drive letters or mount points you wish to scan.
# 3. Optionally change `log_file` and CSV output filenames as needed.
# 4. Run the script; it will log progress, output 'storage_analysis_results.csv', 
#    and display a bar chart of the top 10 largest folders.
# -----------------------------------------------------------------------------


# Set the drive(s) to scan
drives = ["C:\\", "D:\\"]
log_file = "storage_scan_log.txt"

# Function to log messages with UTF-8 encoding
def log_message(message):
    """Logs messages to a file and prints them to the console (supports Unicode)."""
    with open(log_file, "a", encoding="utf-8") as f:  # Use UTF-8 encoding
        f.write(message + "\n")
    print(message)

# Function to calculate folder size
def get_folder_size(path):
    total_size = 0
    try:
        for dirpath, _, filenames in os.walk(path):
            for f in filenames:
                fp = os.path.join(dirpath, f)
                if os.path.exists(fp):
                    total_size += os.path.getsize(fp)
    except PermissionError:
        log_message(f"‚ö†Ô∏è Skipped: {path} (Permission Denied)")
    return total_size / (1024 ** 3)  # Convert bytes to GB

# Start logging
log_message("üöÄ Starting storage scan...\n")

# Scan top-level folders in each drive
storage_data = []
total_folders = 0
start_time = time.time()

for drive in drives:
    if os.path.exists(drive):
        log_message(f"üìÅ Scanning {drive}...")
        folders = os.listdir(drive)
        total_folders += len(folders)

        for i, folder in enumerate(folders):
            folder_path = os.path.join(drive, folder)
            if os.path.isdir(folder_path):  # Only scan directories
                size_gb = get_folder_size(folder_path)
                storage_data.append([drive, folder, size_gb])

            # Progress update every 10 folders
            if i % 10 == 0 or i == len(folders) - 1:
                elapsed_time = time.time() - start_time
                log_message(f"üìä Processed {i+1}/{len(folders)} folders in {drive} (Elapsed: {elapsed_time:.2f}s)")

# Convert to DataFrame
df = pd.DataFrame(storage_data, columns=["Drive", "Folder", "Size (GB)"])

# Sort by size (largest first)
df = df.sort_values(by="Size (GB)", ascending=False)

# Get total disk usage
for drive in drives:
    total, used, free = shutil.disk_usage(drive)
    df.loc[len(df.index)] = [drive, "TOTAL DISK SPACE", total / (1024 ** 3)]
    df.loc[len(df.index)] = [drive, "USED DISK SPACE", used / (1024 ** 3)]
    df.loc[len(df.index)] = [drive, "FREE DISK SPACE", free / (1024 ** 3)]
    log_message(f"‚úÖ {drive} - Used: {used / (1024 ** 3):.2f} GB | Free: {free / (1024 ** 3):.2f} GB")

# Save the results to the log file
df.to_csv("storage_analysis_results.csv", index=False)
log_message("üìÅ Results saved as 'storage_analysis_results.csv'\n")

# Plot the top 10 largest folders
df_filtered = df[df["Folder"] != "TOTAL DISK SPACE"]
df_filtered = df_filtered[df_filtered["Folder"] != "USED DISK SPACE"]
df_filtered = df_filtered[df_filtered["Folder"] != "FREE DISK SPACE"]
df_top10 = df_filtered.head(10)

plt.figure(figsize=(10, 5))
plt.barh(df_top10["Folder"], df_top10["Size (GB)"], color="skyblue")
plt.xlabel("Size (GB)")
plt.ylabel("Folders")
plt.title("Top 10 Largest Folders")
plt.gca().invert_yaxis()  # Largest on top
plt.show()

log_message("‚úÖ Analysis complete!")
